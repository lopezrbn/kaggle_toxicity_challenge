{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85aaab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "ROOT_DIR = Path().resolve().parents[0]\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "import config as cfg\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import load_from_disk, Dataset\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "N_RUN = 3               # Number of run to separe different experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608daf5",
   "metadata": {},
   "source": [
    "# Functions and classes definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26173279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_datasets(ds: Dataset, fold: int) -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "\tSplits the dataset into training and validation sets based on the specified fold.\n",
    "    Args:\n",
    "\t\tds: The dataset to split.\n",
    "\t\tfold: The fold number to use for validation.\n",
    "\tReturns:\n",
    "\t\tds_train: The training dataset excluding the specified fold.\n",
    "\t\tds_val: The validation dataset containing only the specified fold.\n",
    "    \"\"\"\n",
    "    ds_train = ds.filter(lambda x: x[\"fold\"] != fold)\n",
    "    ds_val = ds.filter(lambda x: x[\"fold\"] == fold)\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ec4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_name: str = cfg.MODEL_BASE) -> AutoModelForSequenceClassification:\n",
    "    \"\"\"Initializes the model for sequence classification.\n",
    "    Args:\n",
    "\t\tmodel_name: The name of the pre-trained model to load.\n",
    "\tReturns:\n",
    "\t\tAn instance of AutoModelForSequenceClassification initialized with the specified model.\n",
    "\t\"\"\"\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=6,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90ff17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "    \"\"\" Computes the ROC AUC score for the evaluation predictions.\n",
    "    Args:\n",
    "        eval_pred: A tuple containing the logits and labels.\n",
    "    Returns:\n",
    "        A dictionary containing the ROC AUC score.\n",
    "    \"\"\"\n",
    "    # Unpack the logits and labels from the evaluation predictions\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to probabilities using the sigmoid function\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    # Calculate the ROC AUC score using the probabilities and true labels\n",
    "    auc = roc_auc_score(labels, probs, average=\"macro\")\n",
    "    # Return the ROC AUC score in a dictionary\n",
    "    return {\"roc_auc_macro\": auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879860be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_highlighted_box(text: str, width: int = 80) -> None:\n",
    "    \"\"\"\n",
    "    Prints a highlighted box with the given text centered.\n",
    "    Used mainly to remark the start of a new fold message in the training\n",
    "    process, which writes a lot of text to the console and makes it hard to\n",
    "\tfollow the output.\n",
    "    \"\"\"\n",
    "    spaces = (width - len(text)) // 2\n",
    "    left_spaces = spaces - 1\n",
    "    right_spaces = spaces if (width - len(text)) % 2 else spaces - 1\n",
    "    print(f\"{'-' * width}\")\n",
    "    print(f\"|{' ' * (width - 2)}|\")\n",
    "    print(f\"|{' ' * left_spaces}{text}{' ' * right_spaces}|\")\n",
    "    print(f\"|{' ' * (width - 2)}|\")\n",
    "    print(f\"{'-' * width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132893da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_args(cfg, checkpoint_dir: str) -> TrainingArguments:\n",
    "    \"\"\"\n",
    "    Create a fully configured TrainingArguments instance.\n",
    "    The function is intentionally stateless except for `cfg` and the path.\n",
    "    Args:\n",
    "\t\t- cfg: Configuration object containing training parameters.\n",
    "\t\t- checkpoint_dir: Directory where the model checkpoints will be saved.\n",
    "\tReturns:\n",
    "\t\t- A TrainingArguments instance with all necessary parameters set.\n",
    "    \"\"\"\n",
    "    return TrainingArguments(\n",
    "        # structure\n",
    "        num_train_epochs           \t= cfg.EPOCHS,\n",
    "        per_device_train_batch_size\t= cfg.BATCH_SIZE,\n",
    "        per_device_eval_batch_size \t= cfg.BATCH_SIZE,\n",
    "        gradient_accumulation_steps\t= 1,\n",
    "\n",
    "        # optimisation\n",
    "        learning_rate  \t\t\t\t\t\t\t= cfg.LEARNING_RATE,\n",
    "        weight_decay   \t\t\t\t\t\t\t= 0.01,\n",
    "        optim          \t\t\t\t\t\t\t= \"adamw_torch_fused\",\n",
    "\n",
    "        # evaluation / saving\n",
    "        eval_strategy \t\t\t\t\t\t\t= \"steps\",\n",
    "        eval_steps          \t\t\t\t= cfg.EVAL_STEPS,\n",
    "        save_strategy       \t\t\t\t= \"steps\",\n",
    "        save_steps          \t\t\t\t= cfg.SAVE_STEPS,\n",
    "        load_best_model_at_end \t\t\t= True,\n",
    "        metric_for_best_model  \t\t\t= \"eval_roc_auc_macro\",\n",
    "        save_only_model     \t\t\t\t= True,\n",
    "        save_total_limit    \t\t\t\t= cfg.SAVE_TOTAL_LIMIT,\n",
    "\n",
    "        # precision / memory\n",
    "        fp16                  \t\t\t= True,\n",
    "        gradient_checkpointing\t\t\t= False,\n",
    "        dataloader_num_workers\t\t\t= 2,\n",
    "        dataloader_pin_memory \t\t\t= True,\n",
    "\n",
    "        # logging / reproducibility\n",
    "        logging_steps \t\t\t\t\t\t\t= cfg.LOGGING_STEPS,\n",
    "        seed          \t\t\t\t\t\t\t= cfg.RANDOM_SEED,\n",
    "        output_dir    \t\t\t\t\t\t\t= checkpoint_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df85e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerWithTrainMetrics(Trainer):\n",
    "\t\"\"\"\n",
    "    Custom Trainer class that extends the default Trainer to include training\n",
    "    metrics in the evaluation process, in addition to the standard evaluation metrics.\n",
    "    These metrics will be used to monitor the training performance and plot\n",
    "    training curves.\n",
    "    \"\"\"\n",
    "\tdef evaluate(\n",
    "        self,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ) -> Dict[str, float]:\n",
    "\t\t\"\"\"\n",
    "\t\tEvaluate the model on the given evaluation dataset and also on the training dataset.\n",
    "\t\tThis method extends the default evaluate method to include training metrics.\n",
    "\t\tArgs:\n",
    "\t\t\t- eval_dataset: The dataset to evaluate the model on. If None, uses the training dataset.\n",
    "\t\t\t- ignore_keys: A list of keys to ignore in the evaluation.\n",
    "\t\t\t- metric_key_prefix: A prefix for the metric keys in the returned dictionary.\n",
    "\t\tReturns:\n",
    "\t\t\t- A dictionary containing the evaluation metrics, including training metrics.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Validation metrics (what is usually returned by Trainer.evaluate)\n",
    "\t\tmetrics = super().evaluate(\n",
    "            eval_dataset=eval_dataset,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "        )\n",
    "\n",
    "        # Training metrics (added in this custom Trainer)\n",
    "\t\ttrain_metrics = super().evaluate(\n",
    "            eval_dataset=self.train_dataset,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=\"train\",\n",
    "        )\n",
    "\n",
    "        # Combine metrics\n",
    "\t\tmetrics.update(train_metrics)\n",
    "\t\treturn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f295a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainer(\n",
    "    cfg,\n",
    "    model_init_fn,\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    training_args: TrainingArguments,\n",
    ") -> TrainerWithTrainMetrics:\n",
    "    \"\"\"\n",
    "    Build the customised Trainer with metrics-over-train logic and\n",
    "    early-stopping callback.\n",
    "    Args:\n",
    "\t\tcfg: Configuration object containing training parameters.\n",
    "\t\tmodel_init_fn: Function to initialize the model.\n",
    "\t\ttrain_ds: Training dataset.\n",
    "\t\tval_ds: Validation dataset.\n",
    "\t\ttraining_args: Training arguments for the Trainer.\n",
    "\tReturns:\n",
    "\t\tA TrainerWithTrainMetrics instance configured with the provided parameters.\n",
    "    \"\"\"\n",
    "    return TrainerWithTrainMetrics(\n",
    "        model          = model_init_fn(cfg.MODEL_BASE),\n",
    "        args           = training_args,\n",
    "        train_dataset  = train_ds,\n",
    "        eval_dataset   = val_ds,\n",
    "        compute_metrics= compute_metrics,\n",
    "        callbacks      = [\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=cfg.EARLY_STOP_PATIENCE\n",
    "            )\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340b6ec",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81889902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'fold', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 159571\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_tokenized = load_from_disk(cfg.PATH_DS_TRAIN_TOKENIZED)\n",
    "ds_train_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8b27d",
   "metadata": {},
   "source": [
    "# Cross-Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9db25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3 has already been trained!! Change 'N_RUN' constant at the top of this notebook to 4.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the number of folds defined in the configuration\n",
    "# This allows for cross-validation training, where the model is trained and validated on different subsets of the data.\n",
    "# Each fold will have its own training and validation datasets, and the model will be trained separately for each fold.\n",
    "\n",
    "# First, check whether the run has been trained before\n",
    "path_run_dir = os.path.join(cfg.PATH_CHECKPOINTS, cfg.MODEL_BASE, f\"run_{N_RUN}\")\n",
    "if not os.path.exists(path_run_dir):\n",
    "\tprint(f\"Starting training for run {N_RUN}...\")\n",
    "\n",
    "\tfor fold in range(cfg.N_FOLDS):\n",
    "\t\t# Build the path for the model checkpoints and final model dinamically based on the run and the fold\n",
    "\t\tpath_checkpoint_dir = os.path.join(path_run_dir, f\"fold_{fold}\")\n",
    "\t\tpath_model_final = os.path.join(path_checkpoint_dir, \"model_final\")\n",
    "\n",
    "\t\t# Get the training and validation datasets for the current fold\n",
    "\t\tds_train, ds_val = get_fold_datasets(ds_train_tokenized, fold)\n",
    "\n",
    "\t\t# Print a highlighted box with the fold information\n",
    "\t\tmessage = (\n",
    "\t\t\tf\"FOLD {fold}: TRAIN SIZE: {len(ds_train)} ({len(ds_train)/len(ds_train_tokenized):.2%}),\"\n",
    "\t\t\tf\"VAL SIZE: {len(ds_val)} ({len(ds_val)/len(ds_train_tokenized):.2%})\"\n",
    "\t\t)\n",
    "\t\tprint_highlighted_box(text=message, width=80)\n",
    "\n",
    "\t\t# Initialize the training arguments\n",
    "\t\targs = make_training_args(cfg, path_checkpoint_dir)\n",
    "\n",
    "\t\t# Initialize the trainer with the model, training arguments, datasets, and metrics\n",
    "\t\ttrainer = make_trainer(\n",
    "\t\t\tcfg=cfg, model_init_fn=model_init, train_ds=ds_train,\n",
    "\t\t\tval_ds=ds_val, training_args=args\n",
    "\t\t)\n",
    "\n",
    "\t\t# Train the model\n",
    "\t\ttrain_results = trainer.train()\n",
    "\t\t\n",
    "\t\t# Save log_history of the last training to easily access it later\n",
    "\t\tpath_hist = os.path.join(path_checkpoint_dir, \"log_history.json\")\n",
    "\t\twith open(path_hist, \"w\") as f:\n",
    "\t\t\tjson.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "\t\t# Save the final model\n",
    "\t\ttrainer.save_model(path_model_final)\n",
    "\n",
    "else:\n",
    "\tprint(f\"Run {N_RUN} has already been trained!! Change 'N_RUN' constant at the top of this notebook to {N_RUN+1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d773212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

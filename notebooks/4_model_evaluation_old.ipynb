{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd808690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "ROOT_DIR = Path().resolve().parents[0]\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "import config as cfg\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_RUN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbafc140",
   "metadata": {},
   "source": [
    "# Training analysis across folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2378385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_steps_from_fold(n_run, fold_id):\n",
    "    path_fold_dir = os.path.join(cfg.PATH_CHECKPOINTS, cfg.MODEL_BASE, f\"run_{n_run}\", f\"fold_{fold_id}\")\n",
    "    checkpoints = [int(folder.split(\"checkpoint-\")[-1]) for folder in os.listdir(path_fold_dir) if folder.startswith(\"checkpoint\")]\n",
    "    return sorted(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bb8ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_from_fold(n_run, fold_id):\n",
    "    last_checkpoint = max(get_checkpoint_steps_from_fold(n_run, fold_id))\n",
    "    path_trainer_state = os.path.join(\n",
    "        cfg.PATH_CHECKPOINTS, cfg.MODEL_BASE, f\"run_{n_run}\", f\"fold_{fold_id}\",\n",
    "        f\"checkpoint-{last_checkpoint}\", \"trainer_state.json\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with open(path_trainer_state, \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "        return trainer_state[\"best_metric\"]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path_trainer_state}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9799c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_0_roc_auc_macro': 0.988704817025802,\n",
       " 'fold_1_roc_auc_macro': 0.9882707078896035,\n",
       " 'fold_2_roc_auc_macro': 0.991930075119802,\n",
       " 'fold_3_roc_auc_macro': 0.9900226678585309,\n",
       " 'fold_4_roc_auc_macro': 0.9913572009696786}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best roc_auc_macro from each fold\n",
    "fold_scores = {f\"fold_{fold_id}_roc_auc_macro\": get_best_metric_from_fold(N_RUN, fold_id) for fold_id in range(cfg.N_FOLDS)}\n",
    "fold_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33d21c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean roc_auc_macro across all folds: 0.9900570937726835\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean roc_auc_macro across all folds\n",
    "print(f\"Mean roc_auc_macro across all folds: {np.mean(list(fold_scores.values()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adf4e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std dev roc_auc_macro across all folds: 0.0014296398089953676\n"
     ]
    }
   ],
   "source": [
    "# Calculate the std dev of the roc_auc_macro across all folds\n",
    "print(f\"Std dev roc_auc_macro across all folds: {np.std(list(fold_scores.values()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f5f2c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b47810",
   "metadata": {},
   "source": [
    "The ROC AUC macro scores across the five folds are very high, with a mean of approximately 0.990057.\n",
    "\n",
    "Moreover, the small standard deviation (~0.001430) indicates minimal variation between the folds. This suggests not only that the model performs well when generalizing, but also that these strong results are not dependent on any particular data split.\n",
    "\n",
    "We can conclude that the model generalizes exceptionally well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52ddbd",
   "metadata": {},
   "source": [
    "# Learning and ROC-AUC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699974c",
   "metadata": {},
   "source": [
    "## Calculate data for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d289bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_datasets(ds, fold):\n",
    "    ds_train = ds.filter(lambda x: x[\"fold\"] != fold)\n",
    "    ds_val = ds.filter(lambda x: x[\"fold\"] == fold)\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57bc6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_checkpoint(n_run, fold_id, step, ds_train, ds_val):\n",
    "    \n",
    "    def _compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "        auc = roc_auc_score(labels, probs, average=\"macro\")\n",
    "        return {\"roc_auc_macro\": auc}\n",
    "\n",
    "    # Load the model trained at given checkpoint\n",
    "    # path_checkpoint = f\"checkpoints/{cfg.MODEL_BASE}/run_{N_RUN}/fold_{fold_id}/checkpoint-{step}\"\n",
    "    path_checkpoint_dir = os.path.join(cfg.PATH_CHECKPOINTS, f\"run_{n_run}\", f\"fold_{fold_id}\", f\"checkpoint-{step}\")\n",
    "    model_trained = AutoModelForSequenceClassification.from_pretrained(path_checkpoint_dir).to(\"cuda\")\n",
    "\n",
    "    # Instantiate TrainingArguments and Trainer for evaluation\n",
    "    eval_args = TrainingArguments(\n",
    "        # output_dir=\"./tmp_eval\",\n",
    "        per_device_eval_batch_size=cfg.BATCH_SIZE,\n",
    "        dataloader_num_workers=2,\n",
    "        fp16=True,\n",
    "        save_strategy=\"no\",\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "    eval_trainer = Trainer(\n",
    "        model=model_trained,\n",
    "        args=eval_args,\n",
    "        compute_metrics=_compute_metrics\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the train and validation datasets\n",
    "    metrics_train = eval_trainer.evaluate(eval_dataset=ds_train)\n",
    "    metrics_val = eval_trainer.evaluate(eval_dataset=ds_val)\n",
    "\n",
    "    return {\n",
    "        \"fold_id\": fold_id,\n",
    "        \"n_run\": n_run,\n",
    "        \"step\": step,\n",
    "        \"train_loss\": metrics_train[\"eval_loss\"],\n",
    "        \"val_loss\": metrics_val[\"eval_loss\"],\n",
    "        \"train_roc_auc_macro\": metrics_train[\"eval_roc_auc_macro\"],\n",
    "        \"val_roc_auc_macro\": metrics_val[\"eval_roc_auc_macro\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceee190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 0...\n",
      "\tEvaluating step 2048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4988' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 16:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEvaluating step 4096...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4988' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 17:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEvaluating step 6144...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2902' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2902/3990 09:56 < 03:43, 4.87 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the points for the plot from the checkpoints if they haven't been\n",
    "# calculated before, or load them otherwise.\n",
    "\n",
    "if not os.path.exists(cfg.PATH_DF_SCORES):\n",
    "    \n",
    "    # Initialize var\n",
    "    scores = []\n",
    "\n",
    "    # Load the tokenized dataset\n",
    "    ds_train_tokenized = load_from_disk(cfg.PATH_DS_TRAIN_TOKENIZED)\n",
    "    \n",
    "    # Loop the folds to evaluate all the checkpoints steps\n",
    "    for fold_id in range(cfg.N_FOLDS):\n",
    "        print(f\"Evaluating fold {fold_id}...\")\n",
    "        # Get the train/validation splits\n",
    "        ds_train, ds_val = get_fold_datasets(ds_train_tokenized, fold_id)\n",
    "        # Get all the checkpoints steps in the run\n",
    "        steps = get_checkpoint_steps_from_fold(N_RUN, fold_id)\n",
    "        for step in steps:\n",
    "            print(f\"\\tEvaluating step {step}...\")\n",
    "            scores.append(eval_checkpoint(fold_id, N_RUN, step, ds_train, ds_val))\n",
    "\n",
    "    # Calculate the number of steps per epoch, to then calculate the epoch\n",
    "    steps_per_epoch = math.ceil(len(ds_train) / cfg.BATCH_SIZE)\n",
    "\n",
    "    # Create a df with the results\n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    df_scores[\"epoch\"] = df_scores[\"step\"] / steps_per_epoch\n",
    "    df_scores.insert(2, \"epoch\", df_scores.pop(\"epoch\"))\n",
    "    df_scores.to_csv(cfg.PATH_DF_SCORES, index=False)\n",
    "\n",
    "else:\n",
    "    df_scores = pd.read_csv(cfg.PATH_DF_SCORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295a124",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75871f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id = 0\n",
    "df_filtered = df_scores[df_scores[\"fold_id\"] == fold_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss vs Epoch Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_filtered[\"epoch\"], df_filtered[\"train_loss\"], marker=\"o\", label='Train Loss')\n",
    "plt.plot(df_filtered[\"epoch\"], df_filtered[\"val_loss\"], marker=\"o\", label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4442b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC vs Epoch Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_filtered[\"epoch\"], df_filtered[\"train_roc_auc_macro\"], marker=\"o\", label='Train ROC AUC Macro')\n",
    "plt.plot(df_filtered[\"epoch\"], df_filtered[\"val_roc_auc_macro\"], marker=\"o\", label='Validation ROC AUC Macro')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"ROC AUC Macro\")\n",
    "plt.title(\"ROC AUC Macro vs Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e5d35",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd6875",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887635a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c2615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8044cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762f895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# MODEL = \"microsoft/deberta-v3-large\"\n",
    "MODEL = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "PATH_DF_TRAIN = \"data/original_data/train.csv\"\n",
    "PATH_DF_TEST = \"data/original_data/test.csv\"\n",
    "PATH_DATA_PROCESSED = f\"data/processed_data/{MODEL}\"\n",
    "if not os.path.exists(PATH_DATA_PROCESSED):\n",
    "    os.makedirs(PATH_DATA_PROCESSED, exist_ok=True)\n",
    "PATH_DS_TOKENIZED = os.path.join(PATH_DATA_PROCESSED, \"ds_tokenized\")\n",
    "PATH_OUTPUT_DIR = f\"checkpoints/{MODEL}/\"\n",
    "\n",
    "RANDOM_SEED = 31415"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b08c84",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0515709a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "severe_toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "obscene",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "threat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "insult",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "identity_hate",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "a1b5d3ee-3a87-444c-a91c-b979d8090b91",
       "rows": [
        [
         "0",
         "0000997932d777bf",
         "Explanation\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "000103f0d9cfb60f",
         "D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "2",
         "000113f07ec002fd",
         "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "3",
         "0001b41b1c6bb37e",
         "\"\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "4",
         "0001d958c54c6e35",
         "You, sir, are my hero. Any chance you remember what page that's on?",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "5",
         "00025465d4725e87",
         "\"\n\nCongratulations from me as well, use the tools well.  · talk \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "6",
         "0002bcb3da6cb337",
         "COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK",
         "1",
         "1",
         "1",
         "0",
         "1",
         "0"
        ],
        [
         "7",
         "00031b1e95af7921",
         "Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "8",
         "00037261f536c51d",
         "Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "9",
         "00040093b2687caa",
         "alignment on this subject and which are contrary to those of DuLithgow",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "10",
         "0005300084f90edc",
         "\"\nFair use rationale for Image:Wonju.jpg\n\nThanks for uploading Image:Wonju.jpg. I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use. In addition to the boilerplate fair use template, you must also write out on the image description page a specific explanation or rationale for why using this image in each article is consistent with fair use.\n\nPlease go to the image description page and edit it to include a fair use rationale.\n\nIf you have uploaded other fair use media, consider checking that you have specified the fair use rationale on those pages too. You can find a list of 'image' pages you have edited by clicking on the \"\"my contributions\"\" link (it is located at the very top of any Wikipedia page when you are logged in), and then selecting \"\"Image\"\" from the dropdown box. Note that any fair use images uploaded after 4 May, 2006, and lacking such an explanation will be deleted one week after they have been uploaded, as described on criteria for speedy deletion. If you have any questions please ask them at the Media copyright questions page. Thank you. (talk • contribs • ) \nUnspecified source for Image:Wonju.jpg\n\nThanks for uploading Image:Wonju.jpg. I noticed that the file's description page currently doesn't specify who created the content, so the copyright status is unclear. If you did not create this file yourself, then you will need to specify the owner of the copyright. If you obtained it from a website, then a link to the website from which it was taken, together with a restatement of that website's terms of use of its content, is usually sufficient information. However, if the copyright holder is different from the website's publisher, then their copyright should also be acknowledged.\n\nAs well as adding the source, please add a proper copyright licensing tag if the file doesn't have one already. If you created/took the picture, audio, or video then the  tag can be used to release it under the GFDL. If you believe the media meets the criteria at Wikipedia:Fair use, use a tag such as  or one of the other tags listed at Wikipedia:Image copyright tags#Fair use. See Wikipedia:Image copyright tags for the full list of copyright tags that you can use.\n\nIf you have uploaded other files, consider checking that you have specified their source and tagged them, too. You can find a list of files you have uploaded by following [ this link]. Unsourced and untagged images may be deleted one week after they have been tagged, as described on criteria for speedy deletion. If the image is copyrighted under a non-free license (per Wikipedia:Fair use) then the image will be deleted 48 hours after . If you have any questions please ask them at the Media copyright questions page. Thank you. (talk • contribs • ) \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "11",
         "00054a5e18b50dd4",
         "bbq \n\nbe a man and lets discuss it-maybe over the phone?",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "12",
         "0005c987bdfc9d4b",
         "Hey... what is it..\n@ | talk .\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\n\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "13",
         "0006f16e4e9f292e",
         "Before you start throwing accusations and warnings at me, lets review the edit itself-making ad hominem attacks isn't going to strengthen your argument, it will merely make it look like you are abusing your power as an admin. \nNow, the edit itself is relevant-this is probably the single most talked about event int he news as of late. His absence is notable, since he is the only living ex-president who did not attend. That's certainly more notable than his dedicating an aircracft carrier. \nI intend to revert this edit, in hopes of attracting the attention of an admin that is willing to look at the issue itself, and not throw accusations around quite so liberally. Perhaps, if you achieve a level of civility where you can do this, we can have a rational discussion on the topic and resolve the matter peacefully.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "14",
         "00070ef96486d6f9",
         "Oh, and the girl above started her arguments with me. She stuck her nose where it doesn't belong. I believe the argument was between me and Yvesnimmo. But like I said, the situation was settled and I apologized. Thanks,",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "15",
         "00078f8ce7eb276d",
         "\"\n\nJuelz Santanas Age\n\nIn 2002, Juelz Santana was 18 years old, then came February 18th, which makes Juelz turn 19 making songs with The Diplomats. The third neff to be signed to Cam's label under Roc A Fella. In 2003, he was 20 years old coming out with his own singles \"\"Santana's Town\"\" and \"\"Down\"\". So yes, he is born in 1983. He really is, how could he be older then Lloyd Banks? And how could he be 22 when his birthday passed? The homie neff is 23 years old. 1983 - 2006 (Juelz death, god forbid if your thinking about that) equals 23. Go to your caculator and stop changing his year of birth. My god.\"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "16",
         "0007e25b2121310b",
         "Bye! \n\nDon't look, come or think of comming back! Tosser.",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "17",
         "000897889268bc93",
         "REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "18",
         "0009801bd85e5806",
         "The Mitsurugi point made no sense - why not argue to include Hindi on Ryo Sakazaki's page to include more information?",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "19",
         "0009eaea3325de8c",
         "Don't mean to bother you \n\nI see that you're writing something regarding removing anything posted here and if you do oh well but if not and you can acctually discuss this with me then even better.\n\nI'd like to ask you to take a closer look at the Premature wrestling deaths catagory and the men listed in it, surely these men belong together in some catagory. Is there anything that you think we can do with the catagory besides delting it?",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "20",
         "000b08c464718505",
         "\"\n\n Regarding your recent edits \n\nOnce again, please read WP:FILMPLOT before editing any more film articles.  Your edits are simply not good, with entirely too many unnecessary details and very bad writing.  Please stop before you do further damage. -''''''The '45 \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "21",
         "000bfd0867774845",
         "\"\nGood to know. About me, yeah, I'm studying now.(Deepu) \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "22",
         "000c0dfd995809fa",
         "\"\n\n Snowflakes are NOT always symmetrical! \n\nUnder Geometry it is stated that \"\"A snowflake always has six symmetric arms.\"\" This assertion is simply not true! According to Kenneth Libbrecht, \"\"The rather unattractive irregular crystals are by far the most common variety.\"\" http://www.its.caltech.edu/~atomic/snowcrystals/myths/myths.htm#perfection Someone really need to take a look at his site and get FACTS off of it because I still see a decent number of falsities on this page. (forgive me Im new at this and dont want to edit anything)\"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "23",
         "000c6a3f0cd3ba8e",
         "\"\n\n The Signpost: 24 September 2012 \n\n Read this Signpost in full\n Single-page\n Unsubscribe\n   \n\"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "24",
         "000cfee90f50d471",
         "\"\n\nRe-considering 1st paragraph edit?\nI don't understand the reasons for 's recent edit of this article  not that I'm sure that the data are necessarily \"\"wrong.\"\"  Rather, I'm persuaded that the strategy of introducing academic honors in the first paragraph is an unhelpful approach to this specific subject.  I note that articles about other sitting Justices have been similarly \"\"enhanced;\"\" and I also believe those changes are no improvement.  \n\nIn support of my view that this edit should be reverted, I would invite anyone to re-visit articles written about the following pairs of jurists.\n A1. Benjamin Cardozo\n A2. Learned Hand\n\n B1. John Marshall Harlan\n B2. John Marshall Harlan II\n\nThe question becomes: Would the current version of the Wikipedia article about any one of them  or either pair  be improved by academic credentials in the introductory paragraph?  I think not.\n\nPerhaps it helps to repeat a wry argument Kathleen Sullivan of Stanford Law makes when she suggests that some on the Harvard Law faculty wonder how Antonin Scalia avoided learning what others have managed to grasp about the processes of judging?  I would hope this anecdote gently illustrates the point. \n\nLess humorous, but an even stronger argument is the one Clarence Thomas makes when he mentions wanting to return his law degree to Yale.\n\nAt a minimum, I'm questioning this edit?  It deserves to be reconsidered.   \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "25",
         "000eefc67a2c930f",
         "Radial symmetry \n\nSeveral now extinct lineages included in the Echinodermata were bilateral such as Homostelea, or even asymmetrical such as Cothurnocystis (Stylophora).\n\n-",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "26",
         "000f35deef84dc4a",
         "There's no need to apologize. A Wikipedia article is made for reconciling knowledge about a subject from different sources, and you've done history studies and not archaeology studies, I guess. I could scan the page, e-mail it to you, and then you could ask someone to translate the page.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "27",
         "000ffab30195c5e1",
         "Yes, because the mother of the child in the case against Michael Jackson was studied in here motives and reasonings and judged upon her character just as harshly as Wacko Jacko himself.  Don't tell me to ignore it and incriminate myself.  I am going to continue refuting the bullshit that Jayjg keeps throwing at me.   18:01, 16 Jun 2005 (UTC)",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "28",
         "0010307a3a50a353",
         "\"\nOk. But it will take a bit of work but I can't quite picture it. Do you have an example I can base it on?  the Duck \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "29",
         "0010833a96e1f886",
         "\"== A barnstar for you! ==\n\n  The Real Life Barnstar lets us be the stars\n   \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "30",
         "0011cc71398479c4",
         "How could I post before the block expires?  The funny thing is, you think I'm being uncivil!",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "31",
         "00128363e367d703",
         "Not sure about a heading of 'Fight for Freedom' what will it contain?",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "32",
         "001325b8b20ea8aa",
         "Praise \n\nlooked at this article about 6 months ago -much improved. ]",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "33",
         "001363e1dbe91225",
         "I was able to post the above list so quickly because I already had it in a text file in my hard drive  I've been meaning to get around to updating the sound list for some time now. \nAs far as generating interest  I've spent four years trying to drum up more interest in freely licensed full length classical music. Unfortunately, my attempts failed - I'm still effectively the only one who does it. The classical music wikiproject was not interested, (Wikipedia_talk:WikiProject_Classical_music/Archive_5#Need_help.21Wikipedia_talk:WikiProject_Music/Archive_3#I_could_use_some_helpWikipedia_talk:WikiProject_Music/Archive_2#Raulbot.2C_and_the_music_list) So I really had given up trying to interest others.  \nThe sound list was featured on digg a while back - http://digg.com/music/Wikipedia_has_free_classical_music_downloads . It got 1600 diggs, which is IMO very impressive.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "34",
         "0013a8b1a5f26bcb",
         "\"\nWell, not \"\"before the process\"\" but \"\"before how we do things with subpages\"\" His RfA is listed on NoSeptember's page and you can find it if you look. September 2004 I think. I have my differences with El_C to be sure, but was surprised to see a block, so I left a note. ++: t/c \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "35",
         "00148d055a169b93",
         "\"\n\nNot at all, you are making a straw man argument here. I never claimed O'Donohue had that position, rather that practitioners and researchers in the field ignored the DSM position, which is exactly what the quote says and also something O'Donohue agrees with. \n\nAgain, I was combating the notion that it was a \"\"absurd part\"\" to claim that pedophilia is a sexual orientation. Since many researchers hold this position, it would be unfair to call it absurd. The disorder part is divided in the field, some argue that it is not a disorder at all, some do. At the end of the day, it is a value judgment (as Cantor pointed out earlier in the thread), not a scientific judgement. If we choose to make this value judgment in the article, it should be stated clearly and not pretend to have a scientific basis.   \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "36",
         "00151a9f93c6b059",
         "\"\n\n \"\"Mainland Asia\"\" includes \"\"the lower basin of China's Yangtze River\"\" as well as \"\"Korea\"\".  But being specific is fine too.  I just found a citation for a more comprehensive DNA study by Hammer below, rather than our generarizations and speculation so far. \n\n Citation for \"\"Yayoi culture was brought to Japan by migrants from Korea, who in turn trace their roots to southeast Asia/south China.\"\" \n\n 2005 DNA study by Hammer\n Describes the Yayoi migration from Korea based on the O-SRY(465) genes and other genes with close lineage (haplogroups O-M122 and O-M95).\nReiterates that \"\"the entire O haplogroup has been proposed to have a Southeast Asian origin.\"\"  (Their definition of Southeast Asia includes southern China).  Then hypothesizes that \"\"the dispersals of Neolithic farmers from Southeast Asia also brought haplogroup O lineages to Korea and eventually to Japan.\"\"\n In the concluding paragraph, it states \"\"we propose that the Yayoi Y chromosomes descend from prehistoric farmers that had their origins in southeastern Asia, perhaps going back to the origin of agriculture in this region.\"\"\n Hammer's DNA study is based on a \"\"global sample consisted of > 2,500 males from 39 Asian populations, including six populations sampled from across the Japanese archipelago.\"\"\n \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "37",
         "0015f4aa35ebe9b5",
         "pretty much everyone from warren county/surrounding regions was born at glens falls hospital. myself included. however, i'm not sure this qualifies anyone as being a glens falls native. rachel ray is, i believe, actually from the town of lake luzerne.  —The preceding unsigned comment was added by 70.100.229.154  04:28:57, August 19, 2007 (UTC)",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "38",
         "00169857adbc989b",
         "Hi Explicit, can you block O Fenian for edit-warring on the Giant's Causeway wp. He has made several edits which can only be described as terrorism.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "39",
         "0016e01b742b8da3",
         "Notability of Rurika Kasuga\nA tag has been placed on Rurika Kasuga, requesting that it be speedily deleted from Wikipedia. This has been done because the article seems to be about a person, group of people, band, club, company, or web content, but it does not indicate how or why the subject is notable, that is, why an article about that subject should be included in Wikipedia. Under the criteria for speedy deletion, articles that do not assert notability may be deleted at any time. Please see the guidelines for what is generally accepted as notable, and if you can indicate why the subject of this article is notable, you may contest the tagging. To do this, add  on the top of the page (below the existing db tag) and leave a note on the article's talk page explaining your position. Please do not remove the speedy deletion tag yourself, but don't hesitate to add information to the article that would confirm its subject's notability under the guidelines.\n\nFor guidelines on specific types of articles, you may want to check out our criteria for biographies, for web sites, for bands, or for companies. Feel free to leave a note on my talk page if you have any questions about this.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "40",
         "001735f961a23fc4",
         "\"\n Sure, but the lead must briefly summarize Armenia's history. I simply added what I found necessary. If anyone thinks this or that sentence is redundant for the lead, they are welcome to remove make edits.  talk  \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "41",
         "00173958f46763a2",
         "TFD \n\nI think we just eced. I think we responded to each other without seeing each others responses. I added something in response to yours, but don't know if you saw mine. (T/C//WP:CHICAGO/WP:FOUR)",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "42",
         "001810bf8c45bf5f",
         "You are gay or antisemmitian? \n\nArchangel WHite Tiger\n\nMeow! Greetingshhh!\n\nUh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\n\n1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\n\n2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\n\n3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\n\nBeware of the Dark Side!",
         "1",
         "0",
         "1",
         "0",
         "1",
         "1"
        ],
        [
         "43",
         "00190820581d90ce",
         "FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0"
        ],
        [
         "44",
         "001956c382006abd",
         "I'm Sorry \n\nI'm sorry I screwed around with someones talk page.  It was very bad to do.  I know how having the templates on their talk page helps you assert your dominance over them.  I know I should bow down to the almighty administrators.  But then again, I'm going to go play outside....with your mom.   76.122.79.82",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "45",
         "001b2dd65d9d925c",
         "I don't believe the Lisak criticism present there conforms with the NPV rule.  Lisak doesn't have a neutral point of view to begin with.  If an offer to polygraph or even concerned review of polygraph results shocks a complainant into thinking her lies have been uncovered, the recantation is still perfectly valid.  If you know you are telling the truth, you will argue with machine or investigator.  Also part of Kanin's research was a followup of the recanted story where possible to verify if any were false recantations.  In all followups the recanted version of events matched what the accused said happened.\n\nArguing that Lisak is a respected PHD is baseless if Kanin is a respected PHD.  I agree that my edit wasn't as neutral as possible though, so apologize for that.  Still something must be done here.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "46",
         "001c419c445b5a59",
         "You had a point, and it's now ammended with appropriate encyclopedic notability/significance.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "47",
         "001c557175094f10",
         "In other words, you're too lazy to actually point anything out. Until you change that approach, the tag goes.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "48",
         "001cadfd324f8087",
         "\"\nAs for your claims of \"\"stalking\"\", that is absolute rubbish and serves only to aggravate the situation. I have assumed good faith (and good intentions) on your part, and have never suggested (or seen reason to suggest) that you might have some ulterior motive in mass-adding links to one specific company's web page. Nor, for that matter, have I ever made any suggestion that this is an \"\"administrative\"\" matter or even mentioned such a role. (Clearly, as a party to this disagreement, I would not do so at any rate as it would be a conflict of interest.) I would ask that you thus extend the same good faith toward me, rather than making spurious and unfounded accusations. ''''''chatspy \n\n\"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "49",
         "001d874a4d3e8813",
         "\"::::Jmabel; in regards to predominant scholary consensus who is it that allegedly claims \"\"despite \"\"Third Way\"\" rhetoric, fascism in power functioned rather consistently as a right-wing force\"\"? As far as I'm aware (owning numerous books on the subject) that is not the scholary consensus at all. The consensus, developed by respected scholars of fascism who write in a manner which is not bias to any interest group such as Roger Griffin, Hamish McDonald, Roger Eatwell and Zeev Sternhell all recongise fascism as a \"\"Third Way\"\" as the references show.\n\nThe only dissenters I'm aware of who seem to think fascism has absoutely no leftist connections and is merely a radical right system are street level socialists who want to put as much distance between the movements as possible. This of course does not come from educated people in a position to write books. For example, even the foremost scholary expert on Fascism, and a former member of both the Communist Party and then Socialist Party of Italy, Renzo De Felice doesn't try to \"\"cover up\"\" its socialistic origins and third way status. This is a man who has wrote a definitive seven volume piece on Mussolini. -   \n\n\"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 159571
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(PATH_DF_TRAIN)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d534e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "761a1a62-7e21-44a5-bf7d-4c886494aa4d",
       "rows": [
        [
         "0",
         "00001cee341fdb12",
         "Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,"
        ],
        [
         "1",
         "0000247867823ef7",
         "== From RfC == \n\n The title is fine as it is, IMO."
        ],
        [
         "2",
         "00013b17ad220c46",
         "\" \n\n == Sources == \n\n * Zawe Ashton on Lapland —  /  \""
        ],
        [
         "3",
         "00017563c3f7919a",
         ":If you have a look back at the source, the information I updated was the correct form. I can only guess the source hadn't updated. I shall update the information once again but thank you for your message."
        ],
        [
         "4",
         "00017695ad8997eb",
         "I don't anonymously edit articles at all."
        ],
        [
         "5",
         "0001ea8717f6de06",
         "Thank you for understanding. I think very highly of you and would not revert without discussion."
        ],
        [
         "6",
         "00024115d4cbde0f",
         "Please do not add nonsense to Wikipedia. Such edits are considered vandalism and quickly undone. If you would like to experiment, please use the sandbox instead. Thank you.   -"
        ],
        [
         "7",
         "000247e83dcc1211",
         ":Dear god this site is horrible."
        ],
        [
         "8",
         "00025358d4737918",
         "\" \n Only a fool can believe in such numbers. \n The correct number lies between 10 000 to 15 000. \n Ponder the numbers carefully.  \n\n This error will persist for a long time as it continues to reproduce... The latest reproduction I know is from ENCYCLOPÆDIA BRITANNICA ALMANAC 2008 wich states \n Magnittude: 8.7 (fair enough) \n victims: 70 000 (today 10 000 to 15 000 is not \"\"a lot\"\" so I guess people just come out with a number that impresses enough, I don't know. But I know this: it's just a shameless lucky number that they throw in the air. \n GC \n\n \""
        ],
        [
         "9",
         "00026d1092fe71cc",
         "== Double Redirects == \n\n When fixing double redirects, don't just blank the outer one, you need edit it to point it to the final target, unless you think it's inappropriate, in which case, it needs to be nominated at WP:RfD"
        ],
        [
         "10",
         "0002eadc3b301559",
         "I think its crap that the link to roggenbier is to this article. Somebody that knows how to do things should change it."
        ],
        [
         "11",
         "0002f87b16116a7f",
         "\"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You're just flailing, making up crap on the fly. \n ::: For comparison, the only explicit acknowledgement in the entire Amos Oz article that he is personally Jewish is in the categories!    \n\n \""
        ],
        [
         "12",
         "0003806b11932181",
         ", 25 February 2010 (UTC) \n\n :::Looking it over, it's clear that  (a banned sockpuppet of ) ignored the consensus (&, fwiw, policy-appropriate) choice to leave the page at Chihuahua (Mexico) and the current page should be returned there. Anyone have the time to fix the incoming links? -  18:24"
        ],
        [
         "13",
         "0003e1cccfd5a40a",
         "\" \n\n It says it right there that it IS a type. The \"\"Type\"\" of institution is needed in this case because there are three levels of SUNY schools: \n -University Centers and Doctoral Granting Institutions \n -State Colleges \n -Community Colleges. \n\n It is needed in this case to clarify that UB is a SUNY Center. It says it even in Binghamton University, University at Albany, State University of New York, and Stony Brook University. Stop trying to say it's not because I am totally right in this case.\""
        ],
        [
         "14",
         "00059ace3e3e9a53",
         "\" \n\n == Before adding a new product to the list, make sure it's relevant == \n\n Before adding a new product to the list, make sure it has a wikipedia entry already, \"\"proving\"\" it's relevance and giving the reader the possibility to read more about it. \n Otherwise it could be subject to deletion. See this article's revision history.\""
        ],
        [
         "15",
         "000634272d0d44eb",
         "==Current Position== \n Anyone have confirmation that Sir, Alfred is no longer at the airport and is hospitalised?"
        ],
        [
         "16",
         "000663aff0fffc80",
         "this other one from 1897"
        ],
        [
         "17",
         "000689dd34e20979",
         "== Reason for banning throwing == \n\n This article needs a section on /why/ throwing is banned. At the moment, to a non-cricket fan, it seems kind of arbitrary."
        ],
        [
         "18",
         "000834769115370c",
         ":: Wallamoose was changing the cited material to say things the original source did not say. In response to his objections, I modified the article as we went along. I was not just reverting him. I repeatedly asked him to use the talk page. I've been trying to add to the article for a long time.  It's so thin on content. This is wrong."
        ],
        [
         "19",
         "000844b52dee5f3f",
         "|blocked]] from editing Wikipedia.   |"
        ],
        [
         "20",
         "00084da5d4ead7aa",
         "==Indefinitely blocked== \n I have indefinitely blocked this account."
        ],
        [
         "21",
         "00091c35fa9d0465",
         "== Arabs are committing genocide in Iraq, but no protests in Europe. == \n\n May Europe also burn in hell."
        ],
        [
         "22",
         "000968ce11f5ee34",
         "Please stop. If you continue to vandalize Wikipedia, as you did to Homosexuality, you will be blocked from editing."
        ],
        [
         "23",
         "0009734200a85047",
         "== Energy  == \n\n I have edited the introduction, because previously it said that passive transport does not use any kind of energy. This is not true. Passive transport relies on the kinetic energy of the substance that is being transported. This kinetic energy is what causes it to move around and (by random chance) cross the membrane. The difference is that active transport actually uses the cell's energy (ATP or electrochemical gradient) to pump the substance across the membrane."
        ],
        [
         "24",
         "00097b6214686db5",
         ":yeah, thanks for reviving the tradition of pissing all over articles because you want to live out your ethnic essentialism. Why let mere facts get into the way of enjoying that."
        ],
        [
         "25",
         "0009aef4bd9e1697",
         "MLM Software,NBFC software,Non Banking Financial Company,NBFC software company,NBFC software in india,software for banking,Gold loan software.MLM Software  \n\n '''SEO Services \n Search Engine Optimization \n www.liveindiatech.com \n\n According to a recenBold textt survey people have moved away from searching print media for their needs. They use search engines to find the products and services. The first step to have a successful presence over the internet is creating your own website but that is not enough. When someone searches for the products/services that you offer your name needs to be listed high in the search engine.  \n\n Live India Tech guarantees you search engine optimization using which your organizations name will feature in the top ten listing in all the search engines. This will ensure that the traffic to your site will increase exponentially aiding you in the sales of more products and services.  \n\n We have invested enough resources to develop an SEO system that is far superior to anything else in the market. \n\n www.liveindiatech.com"
        ],
        [
         "26",
         "000a02d807ae0254",
         "@RedSlash, cut it short. If you have sources stating the RoK is sovereign post them. Otherwise please aknowledge WP is not the place to make OR."
        ],
        [
         "27",
         "000a6c6d4e89b9bc",
         "==================== \n Deception is the way of the Ninja..... \n\n Hence, Frank Dux is an amazing Ninja"
        ],
        [
         "28",
         "000bafe2080bba82",
         ". \n\n           Jews are not a race because you can only get it from your mother. Your own mention of Ethiopian Jews not testing \n           as Jews proves it is not, as well as the fact that we accept converts"
        ],
        [
         "29",
         "000bf0a9894b2807",
         ":::If Ollie or others think that one list of the oldest people we know about is too long, the easy answer is to raise the cutoff age. 110 is purely a round number and a full 12 years shorter then the record. We can make it the top 1000 or top 500 or everyone above 115 - tell us what the maximum list size is and we can set a threshold."
        ],
        [
         "30",
         "000c50dceb1eed2b",
         "\" \n *Support Per Jimbo and WP:google \"\"Climatic Research Unit email controversy\"\" =4,930 results, \"\"climategate\"\" =3,210,000 results. MFA  \n\n  \n \""
        ],
        [
         "31",
         "000c9b92318552d1",
         "Professors to the Manhatten Project."
        ],
        [
         "32",
         "000ce41d86f2b886",
         ":::::I have added more wikilinks to my sections and included more secondary sources as you suggested. For all the citations under human disease section, I went through them and exchanged review articles for the primary sources. Thanks again for all your input!"
        ],
        [
         "33",
         "000cf60dbaed8c02",
         "\" \n\n :Not sure whether this is notable enough to be mentioned in the article, but you're right – versions later than 6 (2001) can open files created by all previous versions, but save files only in its own format, with a suffix of \"\".msX\"\", where X is the version number (7, 8, 9, 10, 11, 12...). This is intentional, of course. Besides, all versions but 7 save files in a compressed format. ☭共产主义万岁★ \""
        ],
        [
         "34",
         "000d4f120d5a7303",
         "일이삼사오육칠팔구하고십이요 에헤헤 으헤 으헤 으허허"
        ],
        [
         "35",
         "000d60becb7d1a67",
         "I've deleted the page , as we have no evidence that you are the person named on that page, and its content goes against Wikipedia's policies for the use of user pages."
        ],
        [
         "36",
         "000fc381d4895598",
         "== Nation Radio - request for comment == \n\n Hi Dravecky, I'm just contacting a couple of editors with more specialist knowledge to resolve a discussion at Talk:Nation Radio about naming a broadcast area. I'd be interested in your feedback. Thanks"
        ],
        [
         "37",
         "000ff37cf57ab537",
         "\"\"\" (per your user page)\""
        ],
        [
         "38",
         "001068b809feee6b",
         "\" \n\n ==balance== \n This page has one sentence about the basic definition of the word, and a huge amount about the slang/profane uses. Perhaps the former should be extended; is there no information about female dogs available beyond their name? This is an encyclopaedia, not a dictionary.  \n\n  \n i feel that whoever is looking this definition up is very appropiate and should be deleted from wikipedia...IMMEDIATLY. this word is used very often and is also a very \"\"mean\"\" word. i belive that is majorly true. very much so. okay so, the good meaning is a female dog.  BITCH !!!!!!!!!It also stands for the name Brittany Fellows—Preceding unsigned comment added by   •    \n\n ==etymology== \n The word bitch is from the Old Norse Bikkjuna meaning \"\"female of the dog\"\" of unknown origin, Grimm derives the Old Norse words from Lapp Pittja, But OED notes that \"\"the converse is equally possible.\"\" \n The adj. Bitchy was first seen in 1925.  \n The verb meaning to complain in 1930. \n Slang Bitchen \"\"good\"\" is attested to the 1950's \n\n == reclaiming the word bitch == \n The word bitch is actually only offencive in American and Canadian English. In most other English variants bitch maintains it's correct definition - female canine. People have argued that bitch is different because it is used as an insult - but so is \"\"pig\"\", \"\"dog\"\", \"\"cow\"\" and others. These are not considered profane, so why is bitch? As far as I am concerned, there is nothing to \"\"reclaim\"\" as bitch simply means a female canine. \n\n It may be used as a pejorative or descriptor - that does not make it a profanity. \n\n    \n It should definately say something about Kyle Vanderweilen and all his bitchin Is there any particular evidence of women \"\"reclaiming\"\" the word bitch in the 90s? Can anyone point to articles on this, etc.? The song is definitely interesting and belongs here, but doesn't actually reclaim the word \"\"bitch\"\" any more than it reclaims the word \"\"sinner.\"\" \n\n Also, I don't really understand the last paragraph and it sucks  I was going to try and clear it up, but I realized I don't know what it means. Can someone point to a source that lays out the argument about bitches, fertility and patriarchy more clearly? \n\n \n\n :We don't need articles at all as I there are definitely enough examples (even outside of the 90s). Missy Elliot cleary and repeatedly reclaims the word, for instance, \"\"She's a Bitch\"\".   \n\n ::References are fine  just the name \"\"Missy Elliot\"\" got me to find a quote of a rolling stone review that mentioned \"\"reclaiming\"\" the word. In fact, if some one knows more about her, it might be interesting to add a section on her work and on how it relates to women/slurs more generally within hip-hop.   \n\n :::How relevant is this reclamation? It seems to me poison for a woman to use an epithet which still strongly connotes despised traits. I don't see how using a word connoting \"\"querolous\"\", \"\"spiteful\"\", and \"\"malicious\"\" can be empowering. But I don't know; life's a bitch.   \n\n ::::It stems from a reaction to the prevalence of tagging any woman who doesn't adhere to a certain standard of femininity as a bitch. To use a literary example, in the novel The Handmaid's Tale the narrator related the feeling that, whenever she outsmarted a man, she could almost hear him calling her a bitch in his mind, even her own husband. -   \n\n :::::That literary reaction you recited is jealousy; men have names for other men who outsmart them too, but, not having a word like \"\"bitch\"\" for them, they have to resort to a greater variety of pejoritive epithets. Though you haven't been specific, I think that standard of femininity you referred to is a pretext for subordination, which both sexes often strive to impose on the other, but men have traditionally had more power. Because it is such a popular epithet in slang, the connotations of \"\"bitch\"\" are diffuse, though still commonly pernicious. I think bitch, in itself, is not chiefly a denunciation to punish women who do not conform to that standard, but rather a contemptuous word often used for it. \n\n :::::However, I still don't see the point of reclamati"
        ],
        [
         "39",
         "0011c58fcfd6bf91",
         "\" \n\n *@EdJohnston. Your question was about figures in summary. Here are a few points about this. \n #Yes, the number of victims is important in an article about political repression. Therefore, the number (or a range of numbers) should be provided in the introduction. \n #The numbers of people \"\"killed\"\" and the number of people \"\"who died as a result of Communist policies\"\" are different. The second number is significantly greater: it is more than 60 million (rather than 20 million) in the Soviet Union alone according to The Guinness Book of Records. \n #I did not see any estimates of the number of people killed by all Communist regimes except \"\"Black book\"\", which qualifies as a secondary RS written by a group of European historians. It tells exactly this: \"\"100 million people killed by all Communist regimes\"\". They note that the number is approximate, which also should be noted in the introduction. If there are any other secondary RS that tell \"\"N million people killed by all Communist regimes\"\", they must be also used to obtain a range of numbers. \n #As a side note to others, it is grossly inappropriate to discuss sanctions at article talk pages. If you think that sanctions are needed, please go to appropriate administrative noticeboards and report your concerns there.   \""
        ],
        [
         "40",
         "0011cefc680993ba",
         "REDIRECT Talk:Mi Vida Eres Tú"
        ],
        [
         "41",
         "0011ef6aa33d42e6",
         "\" \n I'm not convinced that he was blind. Where is this documented? It's possible that he was just what we'd call \"\"Legally blind\"\" ie didn't have great vision, and that the name \"\"Blind Blake\"\" is an exaggerated moniker. Although I have no proof I've got a feeling that him being legally blind is more likely than totally blind. Of course I've got no evidence to back that thing up.\""
        ],
        [
         "42",
         "0012706ac77a7b37",
         "== Thanks for the Barnstar! == \n\n Thank you for awarding me with the RickK Anti-Vandilism Barnstar!"
        ],
        [
         "43",
         "0012bb72f20ae971",
         "\" \n\n == Ref: SS Ponzi Scheme == \n\n Hi Padillah, \n It is not my opinion that I am trying to impose here, although it may appear to be so. I am just highlighting the fact that it remains controversial whether it is or isn't a Ponzi Scheme (even if a legal one), so you cannot state as a fact that it is not a Ponzi Scheme (I saw the reference, and the perpetrator itself cannot be treated as a \"\"reliable source\"\"). In fact, claiming it is not a Ponzi Scheme seems to be an opinion in itself. My point is that a claim should not be made either way, and the edit in question just accomplishes that.  \n Thank you, \n Virat\""
        ],
        [
         "44",
         "0012bbcbd6958302",
         "\" \n\n Look, Gerry Spence has NOT \"\"never lost a case, criminal or civil.\"\" That is simply untrue. He prevailed in two cases about which he wrote books (Silkwood and the Miss Wyoming vs. Penthouse cases). He won both massively at trial, and both were completely overturned. In 1985 he represented then-15 year old Michael Jones charged with murder as a juvenile. In Oregon the trial was a criminal trial in EVERY sense except that the juvenile neither received a jury (which Spence argued at the time he was entitled to) and the sentencing authority then ended at age 21. Chief Justice Alex Kozinski of the 9Th U.S. Circuit court mentions the arrogance of trial lawyers who claim to \"\"never have lost a cast.\"\"   \"\"n19. There's one famous exception: Gerry Spence, at least to hear him tell it. See Gerry Spence, Gunning for Justice 47 (1982). But see Sherrie F. Nachman, True Lies, Starring Gerry Spence, Am. Law., Sept. 1994, at 13 (disputing Spence's claim made in Gunning for Justice and widely reported by the media, and quoting one victorious Spence adversary, \"\"It's a lie.\"\").\"\"\""
        ],
        [
         "45",
         "00137446b1aec28c",
         "== September 20th Truce == \n\n According to several news sources, a truce was reached in Minsk last night. http://www.bbc.com/news/world-europe-29290246"
        ],
        [
         "46",
         "0013a435effa29bd",
         "I'd never think I'd need to say it, but Wikipedia isn't a fansite discussion board. If anything is unannounced by any authority, it might as well be false. MMORPGs are overrated,"
        ],
        [
         "47",
         "0013be435187e84f",
         "But this is not the article about government position but about the reaction. Add positions to 2008 Kosovo declaration of independence or Foreign relations of Kosovo."
        ],
        [
         "48",
         "0013fed3aeae76b7",
         "DJ Robinson is gay as hell! he sucks his dick so much!!!!!"
        ],
        [
         "49",
         "001411adf8f1dd82",
         "== Dracula's Grandmother == \n\n  \n Dracula's grandmother was a Bulgarian princess, the sister of Ivan Sratzimir. The links with the lands across the Danube remain largely unexamined. I would appreciate any serious contributions. \n (Kaloyan)"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 153164
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>== Throw from out field to home plate. == \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text\n",
       "0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3       00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4       00017695ad8997eb          I don't anonymously edit articles at all.\n",
       "...                  ...                                                ...\n",
       "153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n",
       "153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n",
       "153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n",
       "\n",
       "[153164 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(PATH_DF_TEST)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8141c2",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3617794",
   "metadata": {},
   "source": [
    "## Check nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e0a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9512aa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153164 entries, 0 to 153163\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   id            153164 non-null  object\n",
      " 1   comment_text  153164 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b4c95",
   "metadata": {},
   "source": [
    "No nulls in both train and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379902f0",
   "metadata": {},
   "source": [
    "## Check labels distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358c1cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "0    0.904156\n",
      "1    0.095844\n",
      "Name: proportion, dtype: float64\n",
      "severe_toxic\n",
      "0    0.990004\n",
      "1    0.009996\n",
      "Name: proportion, dtype: float64\n",
      "obscene\n",
      "0    0.947052\n",
      "1    0.052948\n",
      "Name: proportion, dtype: float64\n",
      "threat\n",
      "0    0.997004\n",
      "1    0.002996\n",
      "Name: proportion, dtype: float64\n",
      "insult\n",
      "0    0.950636\n",
      "1    0.049364\n",
      "Name: proportion, dtype: float64\n",
      "identity_hate\n",
      "0    0.991195\n",
      "1    0.008805\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in df_train.columns[2:]:\n",
    "    print(df_train[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325adbbb",
   "metadata": {},
   "source": [
    "Classes in all the labels are highly unbalanced, showing very few positives. It will be needed to stratify train/validation sets with the same distributions for each label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee2ba7",
   "metadata": {},
   "source": [
    "## Check comments' length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0928e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"word_count\"] = df_train[\"comment_text\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c93cccc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "word_count",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "aed44a5c-a357-435a-b4d2-74f8022528bf",
       "rows": [
        [
         "count",
         "159571.0"
        ],
        [
         "mean",
         "67.27352714465661"
        ],
        [
         "std",
         "99.23070219290523"
        ],
        [
         "min",
         "1.0"
        ],
        [
         "25%",
         "17.0"
        ],
        [
         "50%",
         "36.0"
        ],
        [
         "75%",
         "75.0"
        ],
        [
         "max",
         "1411.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    159571.000000\n",
       "mean         67.273527\n",
       "std          99.230702\n",
       "min           1.000000\n",
       "25%          17.000000\n",
       "50%          36.000000\n",
       "75%          75.000000\n",
       "max        1411.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f783b0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train[\"word_count\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d18cbe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "word_count",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "15ab7cf4-a8dd-42a7-862b-a255cbee7884",
       "rows": [
        [
         "count",
         "159571.0"
        ],
        [
         "mean",
         "67.27352714465661"
        ],
        [
         "std",
         "99.23070219290523"
        ],
        [
         "min",
         "1.0"
        ],
        [
         "50%",
         "36.0"
        ],
        [
         "90%",
         "152.0"
        ],
        [
         "95%",
         "230.0"
        ],
        [
         "99%",
         "567.0"
        ],
        [
         "max",
         "1411.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 9
       }
      },
      "text/plain": [
       "count    159571.000000\n",
       "mean         67.273527\n",
       "std          99.230702\n",
       "min           1.000000\n",
       "50%          36.000000\n",
       "90%         152.000000\n",
       "95%         230.000000\n",
       "99%         567.000000\n",
       "max        1411.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"word_count\"].describe(percentiles=[0.9, 0.95, 0.99,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf8c054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 256 --> percentile: 95.95%\n",
      "Word count: 512 --> percentile: 98.76%\n",
      "Word count: 1024 --> percentile: 99.98%\n"
     ]
    }
   ],
   "source": [
    "for max_word_count in [256, 512, 1024]:\n",
    "    print(f\"Word count: {max_word_count} --> percentile: {(df_train['word_count'] <= max_word_count).sum() / len(df_train):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1529273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=[\"word_count\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeae06",
   "metadata": {},
   "source": [
    "Word count will be truncated to a limit of 256, 512 or 1024 words (as powers of 2), depending on the balance between precision/training time/RAM needs, which will be decided later. In any case, with these words limits, most of the cases (>95%) are covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c509b4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce03a35",
   "metadata": {},
   "source": [
    "## Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be6f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "0    0.904156\n",
      "1    0.095844\n",
      "Name: proportion, dtype: float64\n",
      "severe_toxic\n",
      "0    0.990004\n",
      "1    0.009996\n",
      "Name: proportion, dtype: float64\n",
      "obscene\n",
      "0    0.947052\n",
      "1    0.052948\n",
      "Name: proportion, dtype: float64\n",
      "threat\n",
      "0    0.997004\n",
      "1    0.002996\n",
      "Name: proportion, dtype: float64\n",
      "insult\n",
      "0    0.950636\n",
      "1    0.049364\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in df_train.columns[2:-1]:\n",
    "    print(df_train[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff10a0a",
   "metadata": {},
   "source": [
    "As showed above, the distribution of classes is highly unbalanced (many more 0s than 1s), but also very distant from one to another (toxic showing 10% positives but identity_hate less than 1%).\n",
    "\n",
    "This makes mandatory to use a multi-label stratified KFold, as if we'd use a standard one, there is high chance than some folds wouldn't include any positive class from the less frequent labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f2965f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 159568, 159569, 159570])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_idx = df_train.index.values\n",
    "X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cca57b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_train.iloc[:, 2:-1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be7d5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = IterativeStratification(n_splits=5, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ebb93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"fold\"] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_idx, y)):\n",
    "    df_train.loc[val_idx, \"fold\"] = fold\n",
    "df_train.insert(1, \"fold\", df_train.pop(\"fold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec4341a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fold",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "comment_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "severe_toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "obscene",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "threat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "insult",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "identity_hate",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "0b50c131-fc16-4832-af23-7e74af5bbf79",
       "rows": [
        [
         "81660",
         "da706d3432409139",
         "1",
         "I do think so, yes, on both points (Latin loan words and NASCAR). As for unrecognizable, I agree that they might eventually come to understand the word, particularly in context, however it's not part of our cultural milieu. The various gridiron football league articles might offer more insight.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "91196",
         "f3e82ae6a0ce4a78",
         "3",
         "\"\n\nI had attempted to get a response on their \"\"noticeboard\"\".  But, you, they'll respond to.\n\nRestore my comments.\n\n[[ hopiakuta  Please do  sign  your  signature  on your  message.  %7e%7e   Thank You. -]] \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "980",
         "02ae218e901a58f0",
         "2",
         "\"\nOkay, but only if they are truly not needed; same sort of criticism/praise, no quotes, etc. Thank you.  | talk \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "2919",
         "07e1c4058476e215",
         "1",
         "blocked me accusing me for trolling. If he, you and others believe I’m a troll please keep blocking. If Hillary Clinton is a troll then I’m more than happy to be a troll too. If Hillary Clinton is not a troll then I’m not a troll too. If you believe I’m not a troll please first unblock and then block again for the right reasons. After that we would have a base for an evaluation based on the right accusations. Till then my defend line is what Hillary Clinton said “I would like to offer a historic perspective and not to compare Putin with Hitler”. Thanks.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "7785",
         "14b82142bd070ee2",
         "2",
         "user:ihaveapickle  user:208.113.241.125  \nI do infer that these are the same person, & that this person is a  vandal, & that this person is not demonstrating any capability other than  vandalism.\n\n[[ hopiakuta  Please do  sign  your  signature  on your  message.  %7e%7e   Thank You. -]]",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "59598",
         "9f9681b20f5bcff5",
         "2",
         "LOL, That will be awesome to have to track down all new links to the old references... That might be the explanation though.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "15777",
         "29a9366263aa98cf",
         "0",
         "But Hebephiles *are* Pedophiles if the object of attraction is under the legal age of consent. It's not a matter of debate. It's simply a legal reality. The failure to document this is ridiculous and represents a failure of NPOV.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "120971",
         "8736456a53775afe",
         "0",
         "\"\n The issue originally was the word change, which I imagined would be entirely uncontroversial.  A separate issue then became the attacks on me from all sides led by Dave1185, to which I responded angrily.  My anger has then been used to justify the original attacks.  The issue here is still the word change.  The other issue, you can discuss elsewhere if you like.  On the word change, it still seems very simple to me.  Slavishly adopting the wording of the source is wrong.  The word \"\"said\"\" is entirely neutral, while the word \"\"explained\"\" carries connotations that the article should not be carrying.  It doesn't matter at all if the source said \"\"explained\"\".  Insisting that a word must be used because it appears in the original source makes a complete mockery of the concept of writing an encyclopaedia. The aim here is a bit higher than a massive copy and paste exercise, is it not?  Anyway, it seems to me from the source that Lee did not \"\"explain\"\" or \"\"say\"\" but the Singapore government did.  So, where it says \"\"Lee explained...\"\" it could say \"\"The Singapore government said...\"\".  This is accurate and neutral.\n Incidentally I think the protection of this article is a disgrace, and looks to have been done simply to gain the upper hand in this strange dispute.  There is no \"\"persistent sockpuppetry\"\". 2.220.204.70  \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "46760",
         "7cf44f6031c506d4",
         "3",
         "(actually, they're also showing signs that North8000 might be their next target for a campaign of harassment)",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "90580",
         "f25be3b332006499",
         "0",
         "We quote what the police have said, not what you want them to have said. Enough of this.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81660</th>\n",
       "      <td>da706d3432409139</td>\n",
       "      <td>1</td>\n",
       "      <td>I do think so, yes, on both points (Latin loan...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91196</th>\n",
       "      <td>f3e82ae6a0ce4a78</td>\n",
       "      <td>3</td>\n",
       "      <td>\"\\n\\nI had attempted to get a response on thei...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>02ae218e901a58f0</td>\n",
       "      <td>2</td>\n",
       "      <td>\"\\nOkay, but only if they are truly not needed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>07e1c4058476e215</td>\n",
       "      <td>1</td>\n",
       "      <td>blocked me accusing me for trolling. If he, yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7785</th>\n",
       "      <td>14b82142bd070ee2</td>\n",
       "      <td>2</td>\n",
       "      <td>user:ihaveapickle  user:208.113.241.125  \\nI d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59598</th>\n",
       "      <td>9f9681b20f5bcff5</td>\n",
       "      <td>2</td>\n",
       "      <td>LOL, That will be awesome to have to track dow...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15777</th>\n",
       "      <td>29a9366263aa98cf</td>\n",
       "      <td>0</td>\n",
       "      <td>But Hebephiles *are* Pedophiles if the object ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120971</th>\n",
       "      <td>8736456a53775afe</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\n The issue originally was the word change, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46760</th>\n",
       "      <td>7cf44f6031c506d4</td>\n",
       "      <td>3</td>\n",
       "      <td>(actually, they're also showing signs that Nor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90580</th>\n",
       "      <td>f25be3b332006499</td>\n",
       "      <td>0</td>\n",
       "      <td>We quote what the police have said, not what y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  fold  \\\n",
       "81660   da706d3432409139     1   \n",
       "91196   f3e82ae6a0ce4a78     3   \n",
       "980     02ae218e901a58f0     2   \n",
       "2919    07e1c4058476e215     1   \n",
       "7785    14b82142bd070ee2     2   \n",
       "59598   9f9681b20f5bcff5     2   \n",
       "15777   29a9366263aa98cf     0   \n",
       "120971  8736456a53775afe     0   \n",
       "46760   7cf44f6031c506d4     3   \n",
       "90580   f25be3b332006499     0   \n",
       "\n",
       "                                             comment_text  toxic  \\\n",
       "81660   I do think so, yes, on both points (Latin loan...      0   \n",
       "91196   \"\\n\\nI had attempted to get a response on thei...      0   \n",
       "980     \"\\nOkay, but only if they are truly not needed...      0   \n",
       "2919    blocked me accusing me for trolling. If he, yo...      0   \n",
       "7785    user:ihaveapickle  user:208.113.241.125  \\nI d...      0   \n",
       "59598   LOL, That will be awesome to have to track dow...      0   \n",
       "15777   But Hebephiles *are* Pedophiles if the object ...      0   \n",
       "120971  \"\\n The issue originally was the word change, ...      0   \n",
       "46760   (actually, they're also showing signs that Nor...      0   \n",
       "90580   We quote what the police have said, not what y...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \n",
       "81660              0        0       0       0              0  \n",
       "91196              0        0       0       0              0  \n",
       "980                0        0       0       0              0  \n",
       "2919               0        0       0       0              0  \n",
       "7785               0        0       0       0              0  \n",
       "59598              0        0       0       0              0  \n",
       "15777              0        0       0       0              0  \n",
       "120971             0        0       0       0              0  \n",
       "46760              0        0       0       0              0  \n",
       "90580              0        0       0       0              0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb5412c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "fold",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "proportion",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "732afa89-56c7-44a2-8c40-aaa86d615490",
       "rows": [
        [
         "3",
         "0.20000501344229216"
        ],
        [
         "0",
         "0.20000501344229216"
        ],
        [
         "2",
         "0.20000501344229216"
        ],
        [
         "4",
         "0.20000501344229216"
        ],
        [
         "1",
         "0.19997994623083143"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "fold\n",
       "3    0.200005\n",
       "0    0.200005\n",
       "2    0.200005\n",
       "4    0.200005\n",
       "1    0.199980\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the folds were distributed evenly\n",
    "df_train[\"fold\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c34bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = df_train.columns[3:]\n",
    "label_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec1bc5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "fold",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "proportion_toxic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proportion_severe_toxic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proportion_obscene",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proportion_threat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proportion_insult",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proportion_identity_hate",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "966011b5-9636-4e58-818b-ecb687b884b7",
       "rows": [
        [
         "full dataset",
         "0.0958",
         "0.01",
         "0.0529",
         "0.003",
         "0.0494",
         "0.0088"
        ],
        [
         "0",
         "0.0958",
         "0.0097",
         "0.053",
         "0.0029",
         "0.0493",
         "0.0086"
        ],
        [
         "1",
         "0.0959",
         "0.0099",
         "0.053",
         "0.0033",
         "0.0494",
         "0.0086"
        ],
        [
         "2",
         "0.0958",
         "0.0103",
         "0.0529",
         "0.003",
         "0.0493",
         "0.0083"
        ],
        [
         "3",
         "0.0958",
         "0.0097",
         "0.053",
         "0.0029",
         "0.0493",
         "0.0088"
        ],
        [
         "4",
         "0.0958",
         "0.0103",
         "0.053",
         "0.0029",
         "0.0494",
         "0.0097"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proportion_toxic</th>\n",
       "      <th>proportion_severe_toxic</th>\n",
       "      <th>proportion_obscene</th>\n",
       "      <th>proportion_threat</th>\n",
       "      <th>proportion_insult</th>\n",
       "      <th>proportion_identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>full dataset</th>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              proportion_toxic  proportion_severe_toxic  proportion_obscene  \\\n",
       "fold                                                                          \n",
       "full dataset            0.0958                   0.0100              0.0529   \n",
       "0                       0.0958                   0.0097              0.0530   \n",
       "1                       0.0959                   0.0099              0.0530   \n",
       "2                       0.0958                   0.0103              0.0529   \n",
       "3                       0.0958                   0.0097              0.0530   \n",
       "4                       0.0958                   0.0103              0.0530   \n",
       "\n",
       "              proportion_threat  proportion_insult  proportion_identity_hate  \n",
       "fold                                                                          \n",
       "full dataset             0.0030             0.0494                    0.0088  \n",
       "0                        0.0029             0.0493                    0.0086  \n",
       "1                        0.0033             0.0494                    0.0086  \n",
       "2                        0.0030             0.0493                    0.0083  \n",
       "3                        0.0029             0.0493                    0.0088  \n",
       "4                        0.0029             0.0494                    0.0097  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that within each fold, the proportions of labels are the same as in the full dataset\n",
    "check_proportions = pd.DataFrame([df_train[label_cols].mean().round(4)], index=[\"full dataset\"])\n",
    "check_proportions.index.name = \"fold\"\n",
    "check_proportions = pd.concat([check_proportions, df_train.groupby(\"fold\")[label_cols].mean().round(4)])\n",
    "check_proportions.rename(columns=lambda x: f\"proportion_{x}\", inplace=True)\n",
    "check_proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8672c66",
   "metadata": {},
   "source": [
    "Checked that all the folds have the same number of samples, and the proportion of classes for each label is approximately the same than in the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9c5cf",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e1fc6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f99cdba4e9042b289c5cffe9cb132f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f39eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3112dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "def add_label_vector(batch):\n",
    "    batch[\"labels\"] = [list(row) for row in zip(*[batch[col] for col in label_cols])]\n",
    "    batch[\"labels\"] = np.array(batch[\"labels\"], dtype=np.float32)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fe42b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4454922b34407c900cd6eabe4a4c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a0a439d6004b239647046f6b8612bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4051c57daa034828b823aeb35ae64121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/159571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(PATH_DS_TOKENIZED):\n",
    "    ds_raw = Dataset.from_pandas(df_train)\n",
    "    ds_raw = ds_raw.map(add_label_vector, batched=True)\n",
    "    ds_tokenized = ds_raw.map(tokenize, batched=True, remove_columns=[\"comment_text\"]+label_cols)\n",
    "    ds_tokenized.save_to_disk(PATH_DS_TOKENIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92fb60",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70d769f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_datasets(ds, fold):\n",
    "    ds_train = ds.filter(lambda x: x[\"fold\"] != fold)\n",
    "    ds_val = ds.filter(lambda x: x[\"fold\"] == fold)\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c999f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_name=MODEL):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=6,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33024bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    auc = roc_auc_score(labels, probs, average=\"macro\")\n",
    "    return {\"roc_auc_macro\": auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf4ffb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7b0051836a46fca218c6772317d5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/159571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb09c0e64636441ea582818ee352fc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/159571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "|                                                                                                  |\n",
      "|                    FOLD 0: TRAIN SIZE: 127656 (80.00%), VAL SIZE: 31915 (20.00%)                  |\n",
      "|                                                                                                  |\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 45.38 MiB is free. Including non-PyTorch memory, this process has 15.53 GiB memory in use. Of the allocated memory 15.32 GiB is allocated by PyTorch, and 78.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 56\u001b[0m\n\u001b[1;32m     19\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Training structure parameters\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m path_output_dir,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     48\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_init(MODEL),\n\u001b[1;32m     49\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)],\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(path_output_model)\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/accelerate/utils/operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/accelerate/utils/operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1297\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1297\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1309\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1063\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1055\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1056\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1057\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1061\u001b[0m )\n\u001b[0;32m-> 1063\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:507\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    497\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    498\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    499\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m         output_attentions,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    517\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:355\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    348\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m ):\n\u001b[0;32m--> 355\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    364\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:286\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ):\n\u001b[0;32m--> 286\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    295\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:714\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[1;32m    713\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 714\u001b[0m     rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m rel_att\n",
      "File \u001b[0;32m~/ruben/toxicity_classificator/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:812\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    810\u001b[0m     p2c_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39mr_pos \u001b[38;5;241m+\u001b[39m att_span, \u001b[38;5;241m0\u001b[39m, att_span \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    811\u001b[0m     p2c_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(key_layer, pos_query_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 812\u001b[0m     p2c_att \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp2c_att\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp2c_pos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    817\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p2c_att \u001b[38;5;241m/\u001b[39m scale\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mp2c_att\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 45.38 MiB is free. Including non-PyTorch memory, this process has 15.53 GiB memory in use. Of the allocated memory 15.32 GiB is allocated by PyTorch, and 78.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "ds_tokenized = load_from_disk(PATH_DS_TOKENIZED)\n",
    "ds_tokenized\n",
    "\n",
    "for fold in range(5):\n",
    "\n",
    "    path_output_dir = os.path.join(PATH_OUTPUT_DIR, f\"fold_{fold}\")\n",
    "    if not os.path.exists(path_output_dir):\n",
    "        os.makedirs(path_output_dir, exist_ok=True)\n",
    "    path_output_model = os.path.join(path_output_dir, \"model_final\")\n",
    "\n",
    "\n",
    "    train, val = get_fold_datasets(ds_tokenized, fold)\n",
    "    print(\"-\"*100)\n",
    "    print(f\"|{' '*98}|\")\n",
    "    print(f\"|{' '*20}FOLD {fold}: TRAIN SIZE: {len(train)} ({len(train)/len(ds_tokenized):.2%}), VAL SIZE: {len(val)} ({len(val)/len(ds_tokenized):.2%}){' '*18}|\")\n",
    "    print(f\"|{' '*98}|\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        # Training structure parameters\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=32,     # Try to use multiples of 8 for maximum GPU efficiency\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_eval_batch_size=32,      # Try to use multiples of 8 for maximum GPU efficiency\n",
    "        # Optimization parameters\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        # Evaluation and saving parameters\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=2048,                    # Try to use a multiple of the batch size so the evaluation is made on an integer number of full batches\n",
    "        save_steps=2048,                    # Try to use a multiple of the batch size so the evaluation is made on an integer number of full batches\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"roc_auc_macro\",\n",
    "        # Precision and memory parameters\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=False,\n",
    "        dataloader_num_workers=2,\n",
    "        dataloader_pin_memory=True,\n",
    "        # Logging and reproducibility parameters\n",
    "        logging_steps=50,\n",
    "        seed=RANDOM_SEED,\n",
    "        output_dir= path_output_dir,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_init(MODEL),\n",
    "        args=args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(path_output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e4f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3f5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
